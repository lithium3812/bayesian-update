{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference for Data Quality assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kenichi Maeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem and Bayesian Updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability is probability of a certain event occurring given that another certain event has occurred.\n",
    "Probability of event A conditioned on B is denoted by P(A|B). Be aware of joint probability P(A,B), which is the probability of two events A and B occur together. Definition is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(A,B)}{P(B)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayese Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above definition and commutativity of joint probability P(B,A) = P(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(A,B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called Bayes Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian approach sees probability as observer's confidence (subjective probability), and put Bayes Theorem as the basis for all calculation. According to Bayesian Inference, one has a current state of confidence (a priori/prior probability), and then update his confidence everytime he achieves new information (Data). The updated confidence is called a posteriori/posterior probability, and this process is called Bayesian updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given that we have a certain hypothesis H in our mind with a priori probability P(H), we update our confidence upon a new data D to a posteriori probability P(H|D) according to Bayes Thorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(H|D) = \\frac{P(D|H)P(H)}{P(D)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the conditional probability P(D|H) on the right hand side is likelihood of data D when hypothesis H is true. The denominator P(D) is the total probability of data D occuring for all scenarios. That is, it is calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(D) = P(D|H)P(H) + P(D|\\neg H)P(\\neg H)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of Bayesian updating to data accuracy assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this method to evaluate how much we can trust our data, especially those simple and categorical, e.g. headquarters, founded year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up our situation, we have a certain information (e.g. HQ) for some company (e.g. \"AtomLeap\") from a few sources (e.g. webpage of a startup event, business news, company's official website). Since any source is not 100% reliable and our scraper does't always work accurately, we are not sure if the obtained information is correct. And the biggest problem is, sometimes (often) they do not agree to one another (e.g. for HQ of AtomLeap, source A says \"Berlin\", whereas source B says \"Cologne\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose that we know the probability of each source giving correct information** (the central part of this project, we will come back to this later), this probability can be used as the likelihood $P(D|H)$ in Bayesian updating formula. If we obtain a certain information $H$ from a source with $90$% accuracy, then we start with a priori probability $P(H) = 90$. Then everytime we get a new data from different sources, depending whether it agrees with it ($H$) or not ($\\neg H$), we update our confidence for this hypothesis H based on the Bayesian formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Bayesian Updating algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianInference:\n",
    "    def __init__(self, beliefs=None):\n",
    "        self.beliefs = beliefs or dict()      #candidates and our confidences for them\n",
    "        self.history = []      #record of observations\n",
    "    \n",
    "    def update(self, src, observation):\n",
    "        reliability = data[src].get('reliability')     #probability of each source giving the correct information. Need to be given by independent investigations\n",
    "        \n",
    "        def likelihood(src, positive_observation:bool):     #conditional probability\n",
    "            if  positive_observation:\n",
    "                l = reliability\n",
    "            else:\n",
    "                l = 1 - reliability\n",
    "            return l\n",
    "        def total_prob(apriori, likelihood):     #denominator of the Bayes theorem\n",
    "            p = likelihood*apriori + (1-likelihood)*(1-apriori)\n",
    "            return p\n",
    "        \n",
    "        for hypothesis, apriori in self.beliefs.items():         #update confidences of values already observed\n",
    "            positive_observation = (observation == hypothesis)\n",
    "            l = likelihood(src, positive_observation)\n",
    "            p = total_prob(apriori, l)\n",
    "            aposteriori = l*apriori/p\n",
    "            self.beliefs.update({hypothesis: aposteriori})\n",
    "        if observation not in self.beliefs.keys():       #If the value has never been observed, calculate its confidence and add as a new condidate.\n",
    "            apriori = reliability\n",
    "            for entry in self.history:\n",
    "                l = likelihood(entry['source'], False)\n",
    "                p = total_prob(apriori, l)\n",
    "                apriori = l*apriori/p\n",
    "            aposteriori = apriori\n",
    "            self.beliefs.update({observation: aposteriori})\n",
    "        self.history.append({'source': src, 'result': observation})\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the value with the highest confidence at the moment\n",
    "    \"\"\"\n",
    "    def estimate(self):\n",
    "        best_hypothesis = max(self.beliefs, key=self.beliefs.get)\n",
    "        confidence = self.beliefs[best_hypothesis]\n",
    "        return best_hypothesis, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate examples of data from multiple imperfect sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simulate Bayesian Updating with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "\"\"\"\n",
    "Choose all source grades and their reliability. These have to be given from actual investigations.\n",
    "\"\"\"\n",
    "src_grades = {'A': 0.9, 'B': 0.8, 'C': 0.7, 'D': 0.6}\n",
    "\n",
    "\"\"\"\n",
    "Randomly generate a single observation. Specify the reliability of the source and choices of outcomes.\n",
    "Put the true value as the first entity of 'candidates'.\n",
    "\"\"\"\n",
    "def generate_observation(reliability, candidates:list):\n",
    "    weights = [reliability] + [(1-reliability)/(len(candidates)-1)]*(len(candidates)-1)\n",
    "    draw = choice(candidates, p=weights)\n",
    "    return draw\n",
    "\n",
    "\"\"\"\n",
    "Simulate a set of observations from the above code. Specify number of observations and choices of outcomes. \n",
    "You can also change percentage of source grades. If not specified, each grade of sources appear evenly.\n",
    "\"\"\"\n",
    "def generate_data_example(num, candidates, src_grade_weights=None):\n",
    "    data = dict()\n",
    "    src_list = choice(list(src_grades.keys()), size=num, p=src_grade_weights)\n",
    "    for i in range(0, num):\n",
    "        reliability = src_grades[src_list[i]]\n",
    "        observation = generate_observation(reliability, candidates)\n",
    "        src_label = f'S{i+1}'\n",
    "        data[src_label] = {'reliability': reliability, 'observation': observation}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S1': {'reliability': 0.8, 'observation': 'Berlin'},\n",
       " 'S2': {'reliability': 0.6, 'observation': 'Berlin'},\n",
       " 'S3': {'reliability': 0.9, 'observation': 'Berlin'},\n",
       " 'S4': {'reliability': 0.7, 'observation': 'Berlin'},\n",
       " 'S5': {'reliability': 0.7, 'observation': 'Berlin'},\n",
       " 'S6': {'reliability': 0.7, 'observation': 'Munich'},\n",
       " 'S7': {'reliability': 0.6, 'observation': 'Munich'},\n",
       " 'S8': {'reliability': 0.7, 'observation': 'Berlin'},\n",
       " 'S9': {'reliability': 0.6, 'observation': 'Cologne'},\n",
       " 'S10': {'reliability': 0.8, 'observation': 'Berlin'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_data_example(10, ['Berlin', 'Cologne', 'Munich'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One good example for simulation\n",
    "\"\"\"\n",
    "data = {\n",
    "    'S1': {'reliability': 0.6, 'observation': 'Cologne'},\n",
    "    'S2': {'reliability': 0.6, 'observation': 'Berlin'},\n",
    "    'S3': {'reliability': 0.8, 'observation': 'Munich'},\n",
    "    'S4': {'reliability': 0.6, 'observation': 'Berlin'},\n",
    "    'S5': {'reliability': 0.7, 'observation': 'Berlin'},\n",
    "    'S6': {'reliability': 0.9, 'observation': 'Berlin'},\n",
    "    'S7': {'reliability': 0.8, 'observation': 'Berlin'},\n",
    "    'S8': {'reliability': 0.9, 'observation': 'Berlin'},\n",
    "    'S9': {'reliability': 0.6, 'observation': 'Munich'},\n",
    "    'S10': {'reliability': 0.7, 'observation': 'Munich'}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Bayesian Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian = BayesianInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually demonstrate accumulated confidence. Run the next function repeatedly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(data_iter:iter):   #input data as iterator\n",
    "    src = next(data_iter)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    observation = data[src]['observation']\n",
    "    bayesian.update(src, observation)\n",
    "    print(f'Update with {src}: {observation}')\n",
    "    beliefs = bayesian.beliefs\n",
    "    print(f'Hypothesis and confidence: {beliefs}')\n",
    "    fig.clf()\n",
    "    plt.bar(beliefs.keys(), beliefs.values())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update with S10: Berlin\n",
      "Hypothesis and confidence: {'Berlin': 0.9980903882877149, 'Munich': 0.003601547331446105, 'Cologne': 0.014420910319963964}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simulate(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'S1', 'result': 'Berlin'},\n",
       " {'source': 'S2', 'result': 'Berlin'},\n",
       " {'source': 'S3', 'result': 'Berlin'},\n",
       " {'source': 'S4', 'result': 'Berlin'},\n",
       " {'source': 'S5', 'result': 'Berlin'},\n",
       " {'source': 'S6', 'result': 'Munich'},\n",
       " {'source': 'S7', 'result': 'Munich'},\n",
       " {'source': 'S8', 'result': 'Berlin'},\n",
       " {'source': 'S9', 'result': 'Cologne'},\n",
       " {'source': 'S10', 'result': 'Berlin'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Berlin', 0.9980903882877149)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability of each source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, it is crucial for this approach to know the reliability of each source. Considering we have to use so many different sources, it is unrealistic to establish some predictive model to estimate it. So I would like to propose to classify sources into a several types and at least assign reliability to each of those types. Then we can **investigate the reliability of each source type by comparing our data to some data that we can believe (at least almost) certainly correct.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose the following source types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Official websites\n",
    "2. Annual reports\n",
    "3. Company list webpages (e.g. startup list)\n",
    "4. Texts (e.g. news articles)\n",
    "5. Company profile page of data platforms (one company in one page)\n",
    "6. Wikipedia\n",
    "7. Crunchbase\n",
    "8. Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably we have to implement a source type classifier for 3, 4, 5 (not page classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if the classification is difficult, we can just assign the same reliability to all sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of this method is that now we don't have to be worried about the quality of each source/scraping so much anymore. We can just confirm data with as many sources as possible, and no matter how bad each source is, we will reach the correct information at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, our data acquisition paradigm has been passive. We collected information from given data sources, and therefore for many minor companies we only have confirmation from a single source. Instead, I suggest active data acquisition, that is, crawling web and use random sources there for specific data. In the example of AtomLeap's HQ, if you google \"AtomLeap headquarters\" then you will see a lot of pages mentioning this information. We just need to jump in each page and collect the corresponding part until our confidence for this information to be satisfactory high (>98%?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
